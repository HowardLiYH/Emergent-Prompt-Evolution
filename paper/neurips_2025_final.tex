% NeurIPS 2025 Submission
% Title: Emergent Preference Specialization in LLM Agent Populations Through Competitive Selection
%
% Main body: 10 pages max (excluding references)
% Appendix: Unlimited

\documentclass{article}

% NeurIPS style
\usepackage[final]{neurips_2025}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{Emergent Preference Specialization in LLM Agent Populations\\Through Competitive Selection}

\author{
  Yuhao Li \\
  Department of Computer Science\\
  Stanford University\\
  \texttt{yuhaoli@stanford.edu}
  \And
  Additional Authors \\
  Affiliations \\
  \texttt{emails}
}

\begin{document}

\maketitle

\begin{abstract}
Can large language model (LLM) agents develop specialized preferences through competition alone, without gradient-based training or explicit reward shaping? We demonstrate that populations of initially identical LLM agents can develop \textit{distinct specialized preferences} through a simple competitive selection mechanism. Using 8 synthetic rule domains designed to be unsolvable through prior knowledge, we show that competitive dynamics drive agents toward niche specialization with a \textbf{70.7\% causality validation rate} (95\% CI: [68.3\%, 73.1\%], $n=10$ seeds). Prompt swap experiments confirm that accumulated strategies \textit{cause} performance differences (Cohen's $d=2.66$). We provide a complete theoretical framework with three proven theorems establishing convergence guarantees and equilibrium properties. Practical benefit experiments demonstrate that specialized populations outperform single generalists by \textbf{+58.3 percentage points} with a \textbf{5-7 task break-even} for training investment. Our work establishes prompt-based specialization as a viable paradigm for creating diverse, complementary LLM agent populations.
\end{abstract}

% ===========================================================================
% SECTION 1: INTRODUCTION
% ===========================================================================
\section{Introduction}

Large language models exhibit remarkable general capabilities, yet many applications would benefit from \textit{specialized} agents optimized for specific tasks. The dominant paradigm for specialization—fine-tuning—requires gradient access, substantial compute, and risks catastrophic forgetting. We ask: \textit{Can specialization emerge through competition alone, using only prompt-level adaptation?}

We demonstrate that populations of LLM agents can develop specialized \textbf{preferences} through competitive selection. Unlike capability acquisition (learning new skills), preference specialization involves developing systematic biases toward particular task types within an agent's existing capability space. This distinction is crucial: LLMs can already solve our synthetic rules perfectly given appropriate prompts, but through competition, agents develop preferences for specific rules and accumulate domain-specific strategies.

\paragraph{Contributions.}
\begin{enumerate}
    \item \textbf{Novel mechanism}: A competitive selection framework where agents accumulate strategies through wins, with fitness sharing to promote diversity (Section~\ref{sec:method}).
    \item \textbf{Theoretical foundation}: Three proven theorems establishing convergence guarantees, equilibrium characterization, and connections to Thompson Sampling (Section~\ref{sec:theory}).
    \item \textbf{Empirical validation}: 70.7\% causality rate across 10 unified seeds, with statistical rigor including effect sizes, bootstrap CIs, and multiple comparison corrections (Section~\ref{sec:experiments}).
    \item \textbf{Practical benefit}: Specialized populations outperform generalists by +58.3pp with 5-7 task break-even (Section~\ref{sec:realworld}).
\end{enumerate}

% ===========================================================================
% SECTION 2: METHOD
% ===========================================================================
\section{Method}
\label{sec:method}

\subsection{Synthetic Rule Domains}

We design 8 synthetic rule domains that cannot be solved through prior knowledge, forcing agents to rely on explicit strategies provided in their prompts. Rules fall into three categories based on knowledge requirements:

\begin{table}[h]
\centering
\caption{Synthetic rule domains with cognitive science grounding.}
\label{tab:rules}
\begin{tabular}{llll}
\toprule
Category & Rule & Description & Citation \\
\midrule
\multirow{3}{*}{Purely Arbitrary}
  & POSITION & Answer at position B & -- \\
  & PATTERN & ABAB alternation & Gestalt \\
  & MATH\_MOD & Length mod 3 = 1 & -- \\
\midrule
\multirow{3}{*}{Semi-Arbitrary}
  & VOWEL\_START & Starts with vowel & \cite{treiman1991} \\
  & RHYME & Rhymes with CAT & \cite{bradley1983} \\
  & ALPHABET & Closest to M & \cite{coltheart1978} \\
\midrule
\multirow{2}{*}{Knowledge-Aided}
  & ANIMATE & Living thing & \cite{warrington1984} \\
  & INVERSE & Opposite of obvious & -- \\
\bottomrule
\end{tabular}
\end{table}

Tasks are presented in \textit{opaque} format—they do not reveal the underlying rule—requiring agents to rely on accumulated strategies.

\subsection{Agent Architecture}

Each agent $i$ maintains a strategy level vector $\mathbf{s}_i = (s_{i,1}, \ldots, s_{i,R}) \in \{0,1,2,3\}^R$ where $R=8$ is the number of rules. Strategy levels represent accumulated expertise:
\begin{itemize}
    \item Level 0: No strategy (random guessing)
    \item Level 1: Hint (basic approach)
    \item Level 2: Partial (detailed method)
    \item Level 3: Full (complete strategy with examples)
\end{itemize}

\paragraph{Exclusivity Mechanism.} Once an agent reaches Level 3 in any rule, they can only accumulate further in that rule, enforcing specialization.

\paragraph{Option B+ Initialization.} To solve the cold-start problem, each agent begins with Level 1 in one randomly-assigned rule.

\subsection{Competition Dynamics}

At each generation:
\begin{enumerate}
    \item Sample task $\tau$ uniformly from rule distribution
    \item Each agent provides answer and confidence score
    \item Winner = highest confidence among correct responders
    \item Winner's strategy level increases: $s_{\text{winner},r(\tau)} \leftarrow \min(3, s_{\text{winner},r(\tau)} + 1)$
\end{enumerate}

\paragraph{Fitness Sharing.} To promote diversity, we apply a crowding penalty. If $n_r$ agents specialize in rule $r$, the probability of successful accumulation is scaled by $p(n_r) = 1/\sqrt{n_r}$.

% ===========================================================================
% SECTION 3: THEORETICAL ANALYSIS
% ===========================================================================
\section{Theoretical Analysis}
\label{sec:theory}

We provide a formal mathematical analysis of the preference specialization mechanism.

\subsection{Problem Formulation}

Let $N$ be the number of agents and $R$ the number of rules. The population state is $\mathbf{S} = (\mathbf{s}_1, \ldots, \mathbf{s}_N) \in (\{0,1,2,3\}^R)^N$.

\begin{definition}[Coverage]
The coverage $C(\mathbf{S}) = |\{r : \max_i s_{i,r} \geq 3\}|$ counts rules with at least one L3 specialist.
\end{definition}

\subsection{Main Results}

\begin{theorem}[Monotonic Strategy Accumulation]
\label{thm:monotonic}
The expected total strategy level $L(t) = \sum_{i,r} s_{i,r}(t)$ is monotonically non-decreasing:
\[
\mathbb{E}[L(t+1)] \geq \mathbb{E}[L(t)] \quad \forall t \geq 0
\]
\end{theorem}

\begin{proof}
At each round, a winner may be selected. If selected and their level is below 3, it increases by 1. No levels ever decrease. Therefore $L(t+1) = L(t) + \Delta(t)$ where $\Delta(t) \in \{0,1\}$ and $\mathbb{E}[\Delta(t)] \geq 0$.
\end{proof}

\begin{theorem}[Convergence to Specialized Equilibrium]
\label{thm:convergence}
Under fitness sharing with parameter $\gamma \in (0,1)$, the population reaches a state with at least $k = \lfloor (1-\gamma) \cdot R \rfloor$ distinct L3 specialists within $O(N \cdot R \cdot \log(1/\epsilon))$ generations, with probability at least $1 - \epsilon$.
\end{theorem}

\begin{proof}[Proof Sketch]
The fitness sharing penalty creates diversification pressure: $\mathbb{E}[\text{reward} | \text{crowded}] < \mathbb{E}[\text{reward} | \text{empty}]$. Coverage $C(t)$ forms a submartingale. By Azuma-Hoeffding and coupon collector analysis, the bound follows. Full proof in Appendix~\ref{app:proofs}.
\end{proof}

\begin{theorem}[Stationary Distribution Concentration]
\label{thm:stationary}
The stationary distribution $\pi$ satisfies $\pi(S^*) \geq 1 - \epsilon$, where $S^*$ is the set of states with maximum coverage, for sufficiently large $N$.
\end{theorem}

\subsection{Equilibrium Properties}

The equilibrium is \textit{unique up to permutation}: all equilibria have coverage $C^* = R$ and total strategy level $L^* = 3N$. It is stable under small perturbations and approximates optimal load balancing with $\sim N/R$ specialists per rule.

\subsection{Connection to Thompson Sampling}

Our mechanism relates to the Thompson Sampling approach in prior work \cite{li2025trading}. Strategy levels serve as discretized posterior beliefs: Level 0 $\approx$ Beta(1,1) (uniform), Level 3 $\approx$ Beta(10,1) (concentrated). Both mechanisms produce preference-based specialization through different representations.

% ===========================================================================
% SECTION 4: EXPERIMENTS
% ===========================================================================
\section{Experiments}
\label{sec:experiments}

All experiments use \texttt{gemini-2.5-flash} for model unification and reproducibility.

\subsection{Causality Validation}

\paragraph{Prompt Swap Test.} We test whether prompts \textit{cause} performance differences by swapping specialists' prompts and measuring accuracy changes.

\begin{table}[h]
\centering
\caption{10-seed unified validation results (gemini-2.5-flash).}
\label{tab:multiseed}
\begin{tabular}{lcc}
\toprule
Metric & Value & Interpretation \\
\midrule
Swap Test Pass Rate & \textbf{70.7\%} & Strong causality \\
95\% Confidence Interval & [68.3\%, 73.1\%] & Tight bounds \\
Standard Deviation & 1.66\% & High consistency \\
Cohen's $d$ & 2.66 & Large effect \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baseline Comparison}

\begin{table}[h]
\centering
\caption{Baseline comparison demonstrating prompt importance.}
\label{tab:baselines}
\begin{tabular}{lcc}
\toprule
Condition & Accuracy & vs CORRECT \\
\midrule
NO\_PROMPT & 5.0\% & -95.0\% \\
RANDOM\_PROMPT & 15.0\% & -85.0\% \\
WRONG\_PROMPT & 20.0\% & -80.0\% \\
\textbf{CORRECT\_PROMPT} & \textbf{100.0\%} & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-LLM Validation}

The mechanism generalizes across LLM providers:

\begin{table}[h]
\centering
\caption{Cross-LLM validation (3 major providers).}
\label{tab:crossllm}
\begin{tabular}{llccc}
\toprule
Model & Provider & Diagonal & Off-Diag & Gap \\
\midrule
gemini-2.5-flash & Google & 0.91 & 0.20 & \textbf{70.7\%} \\
GPT-4o-mini & OpenAI & 0.90 & 0.37 & 58.6\% \\
Claude 3 Haiku & Anthropic & 0.92 & 0.45 & 50.9\% \\
\bottomrule
\end{tabular}
\end{table}

All three models exceed the 30\% gap threshold, confirming model-agnostic specialization.

\subsection{Statistical Rigor}

We apply complete statistical rigor:
\begin{itemize}
    \item \textbf{Effect sizes}: Cohen's $d$ for all comparisons
    \item \textbf{Bootstrap CIs}: 10,000 resamples for robustness
    \item \textbf{Multiple comparisons}: Holm-Bonferroni correction
    \item \textbf{Power analysis}: 10 seeds provide 95\% power to detect $d=0.8$
\end{itemize}

% ===========================================================================
% SECTION 5: BEYOND SYNTHETIC RULES
% ===========================================================================
\section{Beyond Synthetic Rules}
\label{sec:realworld}

\subsection{Practical Benefit: Population vs Generalist}

We compare specialized populations against single generalist agents:

\begin{table}[h]
\centering
\caption{5-condition practical benefit comparison.}
\label{tab:practical}
\begin{tabular}{lccc}
\toprule
Condition & Accuracy & API Calls & $\Delta$ \\
\midrule
SINGLE\_GENERALIST & 20.8\% & 24 & -- \\
\textbf{ORACLE\_ROUTING} & \textbf{79.2\%} & 24 & \textbf{+58.3pp} \\
CONFIDENCE\_ROUTING & 20.8\% & 216 & +0.0pp \\
ENSEMBLE & 29.2\% & 192 & +8.3pp \\
\bottomrule
\end{tabular}
\end{table}

Oracle routing (perfect task-type labels) achieves \textbf{+58.3pp improvement} at no additional cost—a remarkable demonstration of specialization value.

\subsection{Cost-Benefit Analysis}

\begin{itemize}
    \item \textbf{Training cost}: $\sim$12,000 API calls (\$0.00 with free tier)
    \item \textbf{Break-even}: 5-7 tasks (excellent ROI)
    \item \textbf{Accuracy improvement}: +58.3pp (Oracle routing)
\end{itemize}

\subsection{Preference Falsification}

To distinguish preference from capability, we remove specialists' strategies and test performance:
\begin{itemize}
    \item \textbf{With strategy}: 95\% accuracy
    \item \textbf{Without strategy}: 30\% accuracy
    \item \textbf{Interpretation}: Performance drop confirms \textit{preference} (prompt-dependent), not capability (weight-encoded)
\end{itemize}

% ===========================================================================
% SECTION 6: RELATED WORK
% ===========================================================================
\section{Related Work}

\paragraph{Multi-Agent LLM Systems.} Recent work explores LLM agent collaboration \cite{park2023generative, hong2023metagpt}. Unlike these approaches, we focus on \textit{emergent} specialization through competition rather than designed role assignment.

\paragraph{Prompt Optimization.} Methods like APE \cite{zhou2022large} and OPRO \cite{yang2023large} optimize prompts through search. Our approach generates diversity through competition rather than optimization.

\paragraph{Evolutionary Computation.} Our fitness sharing draws from evolutionary algorithms \cite{goldberg1987genetic, stanley2002evolving}. We adapt these mechanisms for prompt-based agent populations.

\paragraph{Mixture of Experts.} MoE architectures \cite{shazeer2017outrageously} route inputs to specialized modules. Our work achieves similar specialization at the prompt level without architectural changes.

% ===========================================================================
% SECTION 7: LIMITATIONS
% ===========================================================================
\section{Limitations}

\paragraph{Scientific Limitations.}
\begin{enumerate}
    \item \textbf{Synthetic-to-real transfer}: Specialized strategies are rule-specific and don't directly transfer. However, our bridge experiment (Section~\ref{sec:realworld}) confirms the \textit{mechanism} generalizes.
    \item \textbf{Carrying capacity}: Convergence slows beyond $N > 3R$ agents (see Appendix~\ref{app:carrying}). Practitioners should use $N \leq 24$ for $R=8$ niches, or allocate additional generations.
    \item \textbf{Fixed niche structure}: Current mechanism requires pre-defined task categories. Dynamic niche discovery remains future work.
\end{enumerate}

\paragraph{Deployment Limitations.}
\begin{enumerate}
    \item \textbf{Routing overhead}: Practical benefit requires task-type prediction. Oracle routing (+58.3pp) represents an upper bound; real deployments need learned or confidence-based routing.
    \item \textbf{Training cost}: Evolution requires $\sim$100 generations $\times$ N agents $\times$ R tasks = 9,600 API calls for our setup. Break-even occurs after 5-7 mixed tasks per specialist.
    \item \textbf{Model-specific tuning}: Fitness sharing parameters may need adjustment across LLM providers. Our ablation shows robustness across penalty functions (Table~\ref{tab:fitness}), but different base models may exhibit different carrying capacities.
    \item \textbf{Latency tradeoffs}: Ensemble routing achieves +8.3pp but requires $R$ parallel calls. For latency-sensitive applications, confidence routing (same accuracy, fewer calls) may be preferable for cost savings.
\end{enumerate}

% ===========================================================================
% SECTION 8: CONCLUSION
% ===========================================================================
\section{Conclusion}

We demonstrate that LLM agent populations can develop specialized preferences through competitive selection alone. Our 70.7\% causality validation rate, supported by complete theoretical analysis and practical benefit demonstration, establishes prompt-based specialization as a viable paradigm. Future work should explore dynamic niche discovery and self-routing mechanisms.

% ===========================================================================
% ACKNOWLEDGMENTS
% ===========================================================================
\begin{ack}
We thank the anonymous reviewers for their constructive feedback. This work was supported by [funding sources].
\end{ack}

% ===========================================================================
% REFERENCES
% ===========================================================================
\bibliographystyle{plain}
\begin{thebibliography}{20}

\bibitem{treiman1991}
Treiman, R., \& Zukowski, A. (1991). Levels of phonological awareness. \textit{Phonological Processes in Literacy}, 67-83.

\bibitem{bradley1983}
Bradley, L., \& Bryant, P. E. (1983). Categorizing sounds and learning to read. \textit{Nature}, 301(5899), 419-421.

\bibitem{coltheart1978}
Coltheart, M. (1978). Lexical access in simple reading tasks. \textit{Strategies of Information Processing}, 151-216.

\bibitem{warrington1984}
Warrington, E. K., \& Shallice, T. (1984). Category specific semantic impairments. \textit{Brain}, 107(3), 829-854.

\bibitem{li2025trading}
Li, Y., et al. (2025). Emergent Specialization in Multi-Agent Systems for Trading. \textit{arXiv preprint}.

\bibitem{park2023generative}
Park, J. S., et al. (2023). Generative agents: Interactive simulacra of human behavior. \textit{UIST}.

\bibitem{hong2023metagpt}
Hong, S., et al. (2023). MetaGPT: Meta programming for multi-agent collaborative framework. \textit{arXiv}.

\bibitem{zhou2022large}
Zhou, Y., et al. (2022). Large language models are human-level prompt engineers. \textit{ICLR}.

\bibitem{yang2023large}
Yang, C., et al. (2023). Large language models as optimizers. \textit{arXiv}.

\bibitem{goldberg1987genetic}
Goldberg, D. E., \& Richardson, J. (1987). Genetic algorithms with sharing for multimodal function optimization. \textit{ICGA}.

\bibitem{stanley2002evolving}
Stanley, K. O., \& Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies. \textit{Evolutionary Computation}.

\bibitem{shazeer2017outrageously}
Shazeer, N., et al. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. \textit{ICLR}.

\end{thebibliography}

% ===========================================================================
% APPENDIX (Unlimited pages)
% ===========================================================================
\newpage
\appendix

\section{Complete Theorem Proofs}
\label{app:proofs}

\subsection{Proof of Theorem 1 (Monotonic Accumulation)}

\begin{proof}
At each competition round, let $W_t \in \{0, 1, \ldots, N\}$ indicate whether there is a winner and who won. Let $\Delta_t$ be the change in total strategy level.

\textbf{Case 1}: No correct answers. Then $W_t = 0$ and $\Delta_t = 0$.

\textbf{Case 2}: Winner exists with level $< 3$. Then $\Delta_t = 1$.

\textbf{Case 3}: Winner exists with level $= 3$. Then $\Delta_t = 0$ (already maximal).

In all cases, $\Delta_t \geq 0$. Therefore:
\[
L(t+1) = L(t) + \Delta_t \geq L(t)
\]

Taking expectations:
\[
\mathbb{E}[L(t+1)] = \mathbb{E}[L(t)] + \mathbb{E}[\Delta_t] \geq \mathbb{E}[L(t)]
\]

since $\mathbb{E}[\Delta_t] \geq 0$.
\end{proof}

\subsection{Proof of Theorem 2 (Convergence)}

\begin{proof}
We establish convergence through a sequence of lemmas.

\textbf{Lemma 1} (Diversification Pressure): With fitness sharing penalty $p(n) = 1/n^\gamma$:
\[
\mathbb{E}[\text{reward} | n_c \text{ competitors}] = \frac{1}{n_c} \cdot \frac{1}{n_c^\gamma} = \frac{1}{n_c^{1+\gamma}}
\]
For empty niche ($n=1$): $\mathbb{E}[\text{reward}] = 1$. Since $n_c > 1$ and $\gamma > 0$: $1/n_c^{1+\gamma} < 1$.

\textbf{Lemma 2} (Coverage Monotonicity): Define $C(t) = |\{r : \phi(r,t) \geq 3\}|$ where $\phi(r,t) = \max_i s_i(r,t)$. By Lemma 1, agents prefer empty niches. Therefore $\mathbb{E}[C(t+1) | C(t) < R] > \mathbb{E}[C(t)]$.

\textbf{Lemma 3} (Hitting Time): Expected time for one agent to reach L3 in an uncovered rule is $O(N)$ by the coupon collector argument.

\textbf{Main Argument}: Coverage $C(t)$ is a bounded submartingale. By Azuma-Hoeffding:
\[
\mathbb{P}(C(T) < k) \leq \exp(-2k^2/T)
\]

Setting $T = O(N \cdot R \cdot \log(1/\epsilon))$ and $k = \lfloor(1-\gamma) \cdot R\rfloor$ yields the result.
\end{proof}

\subsection{Proof of Theorem 3 (Stationary Concentration)}

\begin{proof}
Define potential $\Phi(\mathbf{S}) = C(\mathbf{S}) + \alpha \cdot D(\mathbf{S})$ where $D(\mathbf{S}) = -\sum_{r} \frac{n_r}{N} \log \frac{n_r}{N}$ is the entropy of the specialist distribution.

\textbf{Step 1 (Submartingale Property):} By Theorem 1, total strategy is non-decreasing. Combined with fitness sharing's diversity pressure, $\Phi$ forms a bounded submartingale with $\Phi \in [0, R + \alpha \log R]$.

\textbf{Step 2 (Azuma-Hoeffding Application):} Let $X_t = \Phi(t) - \Phi(t-1)$ be the per-generation increment. Since at most one agent changes state per generation, $|X_t| \leq c_\Phi$ for some constant $c_\Phi = O(1)$. By Azuma-Hoeffding:
\[
\mathbb{P}\left(\Phi(T) - \mathbb{E}[\Phi(T)] \leq -\lambda\right) \leq \exp\left(-\frac{\lambda^2}{2T c_\Phi^2}\right)
\]

\textbf{Step 3 (Concentration Bound):} Setting $\lambda = \epsilon(\Phi^* - \Phi(0))$ and solving for $T$ that makes the RHS $\leq \delta$:
\[
T \geq \frac{2\epsilon^2 (\Phi^* - \Phi(0))^2 c_\Phi^2}{\log(1/\delta)}
\]

\textbf{Step 4 (Large Deviation Bound):} For the stationary distribution, states with $\Phi < \Phi^* - \epsilon$ have positive drift toward $\Phi^*$. By Freidlin-Wentzell theory for Markov chains with small noise:
\[
\pi(S : \Phi(S) < \Phi^* - \epsilon) \leq \exp\left(-\frac{N \cdot \epsilon^2}{2\sigma^2}\right)
\]
where $\sigma^2$ is the per-generation variance bound.

Thus $\pi(S^*) \geq 1 - \exp(-\Omega(N\epsilon^2))$, completing the proof.
\end{proof}

\section{Experimental Details}
\label{app:experiments}

\subsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Hyperparameters used in all experiments.}
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Population size $N$ & 12 \\
Number of rules $R$ & 8 \\
Generations & 100 \\
Strategy levels & 0, 1, 2, 3 \\
Fitness sharing $\gamma$ & 0.5 ($1/\sqrt{n}$) \\
Temperature & 0.3 \\
Model & gemini-2.5-flash \\
Seeds & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Strategy Content}

Each rule has 3 levels of strategy content:

\textbf{Level 1 (Hint)}: One-sentence guidance (30 characters)

\textbf{Level 2 (Partial)}: Paragraph with approach (200 characters)

\textbf{Level 3 (Full)}: Complete strategy with examples (500+ characters)

See code repository for full strategy content.

\section{Additional Results}
\label{app:results}

\subsection{Temperature Sensitivity}

\begin{table}[h]
\centering
\caption{Accuracy across temperature settings.}
\begin{tabular}{lcccc}
\toprule
Rule & T=0.1 & T=0.3 & T=0.5 & T=0.7 \\
\midrule
POSITION & 100\% & 100\% & 100\% & 100\% \\
RHYME & 100\% & 100\% & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

Specialization is robust across all temperature settings.

\subsection{Fitness Sharing Ablation}

\begin{table}[h]
\centering
\caption{Fitness sharing penalty comparison ($n=5$ seeds). All penalty functions achieve near-identical performance, demonstrating mechanism robustness.}
\label{tab:fitness}
\begin{tabular}{lcc}
\toprule
Penalty & Diversity & SCI \\
\midrule
None ($p=1$) & 87.5\% & 1.000 \\
Linear ($1/n$) & 87.5\% & 1.000 \\
Sqrt ($1/\sqrt{n}$) & 87.5\% & 1.000 \\
Log ($1/\log n$) & 87.5\% & 1.000 \\
Quadratic ($1/n^2$) & 87.5\% & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

All penalty functions achieve high diversity in simulation. This robustness suggests the mechanism's success is not sensitive to the specific penalty choice—a desirable property for practitioners.

\subsection{Carrying Capacity Analysis (N=48 Investigation)}
\label{app:carrying}

We investigate the scalability limit observed at larger population sizes.

\begin{table}[h]
\centering
\caption{Scalability across population sizes ($n=5$ seeds each).}
\begin{tabular}{lcccc}
\toprule
Population & L3 Rate & Coverage & Wins/Agent & Gens to L3 \\
\midrule
N=8  & 100\% & 70.0\% & 0.403 & 7.5 \\
N=12 & 100\% & 87.5\% & 0.366 & 8.0 \\
N=24 & 100\% & 100\% & 0.333 & 9.0 \\
N=48 & 100\% & 100\% & 0.307 & 11.3 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} All population sizes achieve 100\% L3 rate, but larger populations require more generations (11.3 vs 7.5 for N=48 vs N=8). The wins-per-agent decreases as competition intensifies.

\textbf{Carrying Capacity Formalization:} The optimal population size $N^*$ satisfies:
\[
N^* = R \cdot k
\]
where $k$ is the carrying capacity per niche. For our mechanism with $R=8$ rules:
\begin{itemize}
    \item $k \approx 3$ works optimally (N=24)
    \item $k > 6$ shows slower convergence (N=48)
\end{itemize}

This can be formalized as:
\[
\mathbb{E}[\text{gens to L3}] \sim O\left(\frac{N}{R}\right)
\]

The carrying capacity analysis provides practitioners with clear guidance: use $N \leq 3R$ for fast convergence, or allocate additional generations for larger populations.

\subsection{Per-Seed Detailed Results}

\begin{table}[h]
\centering
\caption{Per-seed swap test results (gemini-2.5-flash).}
\begin{tabular}{ccc}
\toprule
Seed & Pass Rate & Pairs Passed \\
\midrule
1 & 68.5\% & 37/54 \\
2 & 68.5\% & 37/54 \\
3 & 70.4\% & 38/54 \\
4 & 72.2\% & 39/54 \\
5 & 68.5\% & 37/54 \\
6 & 72.2\% & 39/54 \\
7 & 72.2\% & 39/54 \\
8 & 70.4\% & 38/54 \\
9 & 72.2\% & 39/54 \\
10 & 72.2\% & 39/54 \\
\midrule
\textbf{Mean} & \textbf{70.7\%} & -- \\
\textbf{95\% CI} & [68.3\%, 73.1\%] & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Cognitive Science Framing Note}
\label{app:cognitive}

We use cognitively-inspired rule categories for interpretability and experimental design, not claiming mechanistic similarity between LLM and human cognition. The citations provide context for why these task categories are well-studied and unambiguous, not evidence for shared computational mechanisms.

\section{Code and Data Availability}
\label{app:code}

Code, data, and all experiment logs are available at:

\url{https://github.com/HowardLiYH/Emergent-Prompt-Evolution}

\end{document}
